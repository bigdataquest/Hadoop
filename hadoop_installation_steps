commands :
sudo apt-get update
sudo apt install openjdk-8-jre-headless
sudo addgroup hadoop
sudo adduser  --ingroup hadoop  hduser 
sudo apt-get install ssh
which ssh
which sshd

su hduser
ssh-keygen -t rsa  -P ""
cat  $HOME/.ssh/id_rsa.pub >>$HOME/.ssh/authorized_keys
ssh localhost 
yes
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
tar -xvzf hadoop-3.3.1.tar.gz
sudo mkdir /usr/local/hadoop
sudo mv /home/hduser/hadoop-3.3.1 /usr/local/hadoop 	
"error:hduser is not a  sudoers file"
login into root
su 
password if you dont have passwod 

then go to home user
sudo passwd root
set password

sudo adduser hduser sudo 
sudo su hduser

sudo mv /home/hduser/hadoop-3.3.1/* /usr/local/hadoop

setup java and hadoop path in bashrc
check java version
java -version
to get java path
 sudo update-alternatives --config java
 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java


 /usr/local/hadoop/etc/hadoop/hadoop-env.sh
 /usr/local/hadoop/etc/hadoop/core-site.xml
 /usr/local/hadoop/etc/hadoop/mapred-site.xml
 /usr/local/hadoop/etc/hadoop/hdfs-site.xml


step 1:~/.bashrh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin


step 2: set up JAVA_HOME in hadoop-env.sh
/usr/local/hadoop/etc/hadoop/hadoop-env.sh

step 3: edit core-site.xml
 /usr/local/hadoop/etc/hadoop/core-site.xml
sudo mkdir -p /app/hadoop/tmp
sudo chown hduser:hadoop  /app/hadoop/tmp

<configuration>
<property>
    <name>hadoop.tmp.dir</name>
    <value>/app/hadoop/tmp</value>
    <description>A Base for other temporary dir </description>
</property>
<property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:54310</value>
    <description>The name of the default file system.  Either the
      literal string "local" or a host:port for NDFS.
    </description>
    <final>true</final>
</property>
</configuration>

step 4: edit mapred-site.xml

 /usr/local/hadoop/etc/hadoop/mapred-site.xml
 <property>
    <name>mapred.job.tracker</name>
    <value>localhost:54311</value>
    <final>true</final>
  </property>

step 5: edit hdfs-site.xml
before editing wee need to create two folder for datanode and namenode

sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
sudo mkdir -p /usr/local/hadoop_store/hdfs/datenode

sudo chown -R hduser:hadoop /usr/local/hadoop_store/hdfs

 /usr/local/hadoop/etc/hadoop/hdfs-site.xml
  <property>
    <name>dfs.replication</name>
    <value>1</value>
    <description>Set replication </description>
    <final>true</final>
  </property>
   <property>
    <name>dfs.name.dir</name>
    <value>/usr/local/hadoop_store/hdfs/namenode</value>
    <description>Determines where on the local filesystem the DFS name node
      should store the name table.  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
    <final>true</final>
  </property>

  <property>
    <name>dfs.data.dir</name>
    <value>/usr/local/hadoop_store/hdfs/datanode</value>
    <description>Determines where on the local filesystem an DFS data node
       should store its blocks.  If this is a comma-delimited
       list of directories, then data will be stored in all named
       directories, typically on different devices.
       Directories that do not exist are ignored.
    </description>
    <final>true</final>
  </property>
  
  
  
 hadoop namenode -format
sudo chown -R hduser:hadoop /usr/local/hadoop

 start-all.sh
 check all are running or not by using jps
  
 check all deamons  are running in host
 localhost:9870--Namenode
 localhost:8088 --Resource Manager
  


Resources
https://hadoop.apache.org/docs/r3.3.1/hadoop-project-dist/hadoop-common/SingleCluster.html
